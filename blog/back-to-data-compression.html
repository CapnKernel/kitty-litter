<!DOCTYPE html>
<html>
  <head>
    <title>Back to data compression</title>
    <link rel="stylesheet" href="/style.css">
  </head>
  <body>
    <header>Back to data compression</header>
    <nav>
        <ul>

  <li><a href="/blog/index.html">Blog</a></li>

  <li><a href="/index.html">Welcome</a></li>

</ul>

    </nav>
    <main><p>One of my long time fascinations has been with data compression. I've read a lot about it, tried to keep up with modern techniques, but never really written much usable code.</p>

<p>Recently I decided, partly as an exercise in remaining fluent in C, I'd try to re-implement one of the simplest algorithms I know. I used this method almost 20 years ago to compress payloads for a tiny embedded board which had 256kB RAM and 128kB ROM.</p>

<h2>The algorithm.</h2>

<p>This one works by trying to predict the next byte. If it gets it right, it emits a single <code>1</code> bit to say it got it right. If not, it emits a <code>0</code> bit, followed by the actual byte.</p>

<p>Now, to do this directly, it would be slow, as we'd have to work to deal with writing bits unaligned with bytes.</p>

<p>Instead, what we'll do is process 8 bytes at a time, emit those match flags as a byte, followed by however many literal bytes are required.</p>

<p>In this case, the algorithm is extremely well suited to embedded systems with tiny memory budgets [&lt;64kB].</p>

<h2>The prediction.</h2>

<p>We start with an array initialised to 0s, and a <code>state</code> variable. For each character we:</p>

<ol>
<li>check if table[state] == char</li>
<li>emit match / no-match accordingly</li>
<li>set table[state] = char</li>
<li>update state to hash(state, char)</li>
</ol>

<p>Obviously, it helps if the table is a power of 2 in size [I started with 2^^14]. And the <code>hash</code> function can be as simple as <code>state = (state &lt;&lt; 3) ^ char;</code></p>

<h2>Results</h2>

<p>So once I had the code working and debugged, I tested the compression.</p>

<p>Firstly, unsurprisingly, it's very fast. It compressed the <code>enwik8</code> reference file of 100000000 bytes in about 4 seconds. And can decompress again in about 3.5s.</p>

<p>The compression ratio is, in fact, not too bad, given the simplicity of the algorithm.</p>

<table>
<thead>
<tr>
  <th align="right">size</th>
  <th>filename</th>
</tr>
</thead>
<tbody>
<tr>
  <td align="right">42265732</td>
  <td>enwik8.gz</td>
</tr>
<tr>
  <td align="right">57281515</td>
  <td>enwik8.p4_20</td>
</tr>
<tr>
  <td align="right">57285984</td>
  <td>enwik8.lz4</td>
</tr>
<tr>
  <td align="right">61657971</td>
  <td>enwik8.p4_16</td>
</tr>
<tr>
  <td align="right">61719932</td>
  <td>enwik8.p3_20</td>
</tr>
<tr>
  <td align="right">66232007</td>
  <td>enwik8.p3_16</td>
</tr>
<tr>
  <td align="right">67714233</td>
  <td>enwik8.p4_13</td>
</tr>
<tr>
  <td align="right">71055303</td>
  <td>enwik8.p3_13</td>
</tr>
<tr>
  <td align="right">100000000</td>
  <td>enwik8.txt</td>
</tr>
</tbody>
</table>

<p>What you see here are the "gzip -1" of the file, the "lz4 -1" of the file, and various combinations of 3 or 4 bit hash, and 13, 16, or 20bit state size [and thus table size].</p>

<h2>Findings</h2>

<p>It turns out that changing the shift from 3 to 4 bits improves results for "mostly text" data, whilst slightly harming "mostly binary" data.</p>

<p>Also, I find it impressive that with a modest 1MB memory investment, we can achieve slightly better compression than LZ4, which uses the more sophisticated Lempel-Ziv 77 compression algorithm, and a lot more memory.</p>

<p>Choosing the table size is interesting. A larger table allows you to predict from a larger range of possible contexts [state], but also takes longer to fill. A smaller table/state will fill quicker, and likely be good for highly repetitive data, but clearly doesn't compress as well in our sample case.</p>
</main>
    <footer> Copyright &copy; 2019 Curtis Maloney </footer>
  </body>
</html>
